{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport logging\nimport sys\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom keras import regularizers\nfrom keras import metrics\nfrom keras.layers import Dense, LSTM, Conv1D, Dropout, MaxPooling1D, Flatten, Flatten, GRU, Average\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras.layers.embeddings import Embedding\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import sequence\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nimport emoji\nimport re\nimport glob\nlogging.basicConfig(format='%(levelname)s %(asctime)s: %(message)s', level=logging.INFO)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-13T05:13:18.330002Z","iopub.execute_input":"2022-02-13T05:13:18.330523Z","iopub.status.idle":"2022-02-13T05:13:23.396366Z","shell.execute_reply.started":"2022-02-13T05:13:18.330418Z","shell.execute_reply":"2022-02-13T05:13:23.395483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/newdataset/new-data-set.csv\")\nstopWordsFileContent = open(\"/kaggle/input/stopwords/stopWords.txt\", \"r\", encoding=\"utf16\").readlines()\nstemDictFileContent = open(\"/kaggle/input/stem-dict/stem_dictionary.txt\", \"r\").readlines()\n# print(\"Before\", data.shape)\n# data = data[pd.notnull(data['full_text_without_emoji'])]\n# data = data[pd.notnull(data['label'])]\n# print(\"After:\", data.shape)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.397914Z","iopub.execute_input":"2022-02-13T05:13:23.398258Z","iopub.status.idle":"2022-02-13T05:13:23.494868Z","shell.execute_reply.started":"2022-02-13T05:13:23.398219Z","shell.execute_reply":"2022-02-13T05:13:23.494105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stemDict = {}\nfor line in stemDictFileContent:\n    words = line.split(\"\\t\")\n    stemDict[words[0].strip()] = words[1].strip()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.49638Z","iopub.execute_input":"2022-02-13T05:13:23.496632Z","iopub.status.idle":"2022-02-13T05:13:23.525537Z","shell.execute_reply.started":"2022-02-13T05:13:23.496606Z","shell.execute_reply":"2022-02-13T05:13:23.52475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopWords = []\nfor line in stopWordsFileContent:\n    stopWords.append(line.split(\"\\t\")[0])","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.527167Z","iopub.execute_input":"2022-02-13T05:13:23.52772Z","iopub.status.idle":"2022-02-13T05:13:23.534466Z","shell.execute_reply.started":"2022-02-13T05:13:23.52768Z","shell.execute_reply":"2022-02-13T05:13:23.533605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.535713Z","iopub.execute_input":"2022-02-13T05:13:23.536293Z","iopub.status.idle":"2022-02-13T05:13:23.546157Z","shell.execute_reply.started":"2022-02-13T05:13:23.536254Z","shell.execute_reply":"2022-02-13T05:13:23.54533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(text: str) -> list:\n    # text characters to split is from: https://github.com/madurangasiriwardena/corpus.sinhala.tools\n    emojis = ''.join(emj for emj in emoji.UNICODE_EMOJI.keys())\n    tokens = [token for token in\n            re.split(r'[.…,‌ ¸‚\\\"/|—¦”‘\\'“’´!@#$%^&*+\\-£?˜()\\[\\]{\\}:;–Ê  �‪‬‏0123456789' + emojis + ']', text)\n            if token != \"\" and token not in stopWords]\n    stemmedTokens = []\n    for t in tokens:\n        if(t in stemDict.keys()):\n            stemmedTokens.append(stemDict[t])\n        else:\n            stemmedTokens.append(t)\n    return stemmedTokens\ntokenize(\"අනේ හුකන්න කියපන් උන්ට\")","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.548563Z","iopub.execute_input":"2022-02-13T05:13:23.548896Z","iopub.status.idle":"2022-02-13T05:13:23.558522Z","shell.execute_reply.started":"2022-02-13T05:13:23.548862Z","shell.execute_reply":"2022-02-13T05:13:23.55748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_corpus(corpus: list) -> list:\n    return [tokenize(text) for text in corpus]\ntokenized_corpus = tokenize_corpus([\"අනේ හුකන්න කියපන් උන්ට\", \"පාහර බැල්ලි එනෝ මෙතන මට වැඩ කියාදෙන්න හෙළුවෙන්\"])\ntokenized_corpus","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.560036Z","iopub.execute_input":"2022-02-13T05:13:23.56061Z","iopub.status.idle":"2022-02-13T05:13:23.571093Z","shell.execute_reply.started":"2022-02-13T05:13:23.560573Z","shell.execute_reply":"2022-02-13T05:13:23.570298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_WORD_COUNT = 60 \nDATA_SET_CLASSES = {\n    0: [0, 1],\n    1: [1, 0]\n}","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.572489Z","iopub.execute_input":"2022-02-13T05:13:23.572852Z","iopub.status.idle":"2022-02-13T05:13:23.578095Z","shell.execute_reply.started":"2022-02-13T05:13:23.572818Z","shell.execute_reply":"2022-02-13T05:13:23.577104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_class_to_one_hot_representation(classes: list):\n    return np.array([DATA_SET_CLASSES[cls] for cls in classes])\ntransform_class_to_one_hot_representation([1,0,1])","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.581759Z","iopub.execute_input":"2022-02-13T05:13:23.582162Z","iopub.status.idle":"2022-02-13T05:13:23.589712Z","shell.execute_reply.started":"2022-02-13T05:13:23.582127Z","shell.execute_reply":"2022-02-13T05:13:23.588859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_dictionary(corpus_token: list) -> dict:\n    word_frequency = {}\n    dictionary = {}\n\n    for tweet in corpus_token:\n        for token in tweet:\n            if token in word_frequency:\n                word_frequency[token] += 1\n            else:\n                word_frequency[token] = 1\n\n    frequencies = list(word_frequency.values())\n    unique_words = list(word_frequency.keys())\n\n    # sort words by its frequency\n    frequency_indexes = np.argsort(frequencies)[::-1]  # reverse for descending\n    for index, frequency_index in enumerate(frequency_indexes):\n        # 0 is not used and 1 is for UNKNOWN\n        dictionary[unique_words[frequency_index]] = index + 2\n\n    return dictionary\nprint(tokenized_corpus)\ndictionary = build_dictionary(tokenized_corpus)\ndictionary","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.592008Z","iopub.execute_input":"2022-02-13T05:13:23.592717Z","iopub.status.idle":"2022-02-13T05:13:23.606093Z","shell.execute_reply.started":"2022-02-13T05:13:23.592674Z","shell.execute_reply":"2022-02-13T05:13:23.605282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_to_dictionary_values(corpus_token: list, dictionary: dict) -> list:\n    x_corpus = []\n    for tweet in corpus_token:\n        # 1 is for unknown (not in dictionary)\n        x_corpus.append([dictionary[token] if token in dictionary else 1 for token in tweet])\n\n    return x_corpus\ntransform_to_dictionary_values(tokenized_corpus, dictionary)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.609215Z","iopub.execute_input":"2022-02-13T05:13:23.609479Z","iopub.status.idle":"2022-02-13T05:13:23.618345Z","shell.execute_reply.started":"2022-02-13T05:13:23.609447Z","shell.execute_reply":"2022-02-13T05:13:23.617381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_set = data.values","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.620696Z","iopub.execute_input":"2022-02-13T05:13:23.620982Z","iopub.status.idle":"2022-02-13T05:13:23.627843Z","shell.execute_reply.started":"2022-02-13T05:13:23.620957Z","shell.execute_reply":"2022-02-13T05:13:23.627001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_SET_TEXT = 1\nlogging.info(\"Tokenizing the corpus\")\ncorpus_token = tokenize_corpus(data_set[:, DATA_SET_TEXT])","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:23.629428Z","iopub.execute_input":"2022-02-13T05:13:23.630018Z","iopub.status.idle":"2022-02-13T05:13:24.139004Z","shell.execute_reply.started":"2022-02-13T05:13:23.629981Z","shell.execute_reply":"2022-02-13T05:13:24.138008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.info(\"Building the dictionary\")\ndictionary = build_dictionary(corpus_token)\ndictionary_length = len(dictionary) + 2  # 0 is not used and 1 is for UNKNOWN\ndictionary_length","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:24.140539Z","iopub.execute_input":"2022-02-13T05:13:24.140921Z","iopub.status.idle":"2022-02-13T05:13:24.177756Z","shell.execute_reply.started":"2022-02-13T05:13:24.140881Z","shell.execute_reply":"2022-02-13T05:13:24.176883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.info(\"Transforming the corpus to dictionary values\")\nx_corpus = transform_to_dictionary_values(corpus_token, dictionary)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:24.179001Z","iopub.execute_input":"2022-02-13T05:13:24.179352Z","iopub.status.idle":"2022-02-13T05:13:24.202019Z","shell.execute_reply.started":"2022-02-13T05:13:24.179304Z","shell.execute_reply":"2022-02-13T05:13:24.201325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_SET_CLASS = 2\ny_corpus = transform_class_to_one_hot_representation(data_set[:, DATA_SET_CLASS])\ny_corpus","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:24.203011Z","iopub.execute_input":"2022-02-13T05:13:24.203251Z","iopub.status.idle":"2022-02-13T05:13:24.220776Z","shell.execute_reply.started":"2022-02-13T05:13:24.203228Z","shell.execute_reply":"2022-02-13T05:13:24.219783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_corpus = sequence.pad_sequences(x_corpus, maxlen=MAX_WORD_COUNT )\nprint(len(x_corpus))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:24.222138Z","iopub.execute_input":"2022-02-13T05:13:24.222745Z","iopub.status.idle":"2022-02-13T05:13:24.285736Z","shell.execute_reply.started":"2022-02-13T05:13:24.222708Z","shell.execute_reply":"2022-02-13T05:13:24.28481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ################## Deep Neural Network ###################### #\nFOLDS_COUNT = 5\nMAX_EPOCHS = 5\nVALIDATION_TEST_SIZE = 0.12\nmax_word_count = MAX_WORD_COUNT\n\n# splitting data for 5-fold cross validation\nk_fold = StratifiedKFold(n_splits=FOLDS_COUNT, shuffle=True, random_state=18)\n# to split, raw format (integer) is required\ny_corpus_raw = [0 if cls[1] == 1 else 1 for cls in y_corpus]","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:24.28679Z","iopub.execute_input":"2022-02-13T05:13:24.28703Z","iopub.status.idle":"2022-02-13T05:13:24.298367Z","shell.execute_reply.started":"2022-02-13T05:13:24.287006Z","shell.execute_reply":"2022-02-13T05:13:24.297381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LSTM Model","metadata":{}},{"cell_type":"code","source":"fold = 0\ncv_scores = []\nfor train_n_validation_indexes, test_indexes in k_fold.split(x_corpus, y_corpus_raw):\n    x_train_n_validation = x_corpus[train_n_validation_indexes]\n    y_train_n_validation = y_corpus[train_n_validation_indexes]\n    x_test = x_corpus[test_indexes]\n    y_test = y_corpus[test_indexes]\n\n    # train and validation data sets\n#     x_train, x_valid, y_train, y_valid = train_test_split(x_train_n_validation, y_train_n_validation,\n#                                                           test_size=VALIDATION_TEST_SIZE, random_state=94)\n\n    n_timesteps = len(x_train_n_validation)\n    n_features = len(x_train_n_validation[0])\n    # ################## Deep Neural Network Model ###################### #\n    model = Sequential()\n    model.add(Embedding(input_dim=dictionary_length, output_dim=60, input_length=max_word_count))\n    model.add(LSTM(600))\n    model.add(Dense(250, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    adam_optimizer = Adam(lr=0.001, decay=0.0001)\n    model.compile(loss='categorical_crossentropy', optimizer=adam_optimizer, metrics=[metrics.CategoricalAccuracy(), metrics.AUC(), metrics.Precision(), metrics.Recall()])\n    \n#     print(model.summary())\n    # ################## Deep Neural Network Model ###################### #\n\n    best_accuracy = 0\n    best_loss = 100000\n    best_epoch = 0\n\n    epoch_history = {\n        'accuracy': [],\n        'val_accuracy': [],\n        'loss': [],\n        'val_loss': [],\n        'auc': [],\n        'val_auc': []\n    }\n    history = model.fit(x=x_train_n_validation, y=y_train_n_validation, epochs=MAX_EPOCHS, batch_size=32,\n                            verbose=1, shuffle=True)\n    scores = model.evaluate(x_test, y_test, verbose=0)\n    print(model.metrics_names)\n    print(\"scores:\", scores)\n    cv_scores.append(scores)\n    fold += 1","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:13:24.299919Z","iopub.execute_input":"2022-02-13T05:13:24.300569Z","iopub.status.idle":"2022-02-13T05:15:21.98565Z","shell.execute_reply.started":"2022-02-13T05:13:24.30053Z","shell.execute_reply":"2022-02-13T05:15:21.98473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_accuracy = 0\nmean_auc = 0\nmean_precision = 0\nmean_recall = 0\nfor score in cv_scores:\n    mean_accuracy += score[1]\n    mean_auc += score[2]\n    mean_precision += score[3]\n    mean_recall += score[4]\nmean_accuracy = mean_accuracy / FOLDS_COUNT\nmean_auc = mean_auc / FOLDS_COUNT\nmean_precision = mean_precision / FOLDS_COUNT\nmean_recall = mean_recall / FOLDS_COUNT\nmean_f_score = 2*mean_precision*mean_recall/(mean_precision + mean_recall)\nprint(\"mean_accuracy\", mean_accuracy)\nprint(\"mean_auc\", mean_auc)\nprint(\"mean_precision\", mean_precision)\nprint(\"mean_recall\", mean_recall)\nprint(\"mean_f_score\", mean_f_score)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:15:21.987211Z","iopub.execute_input":"2022-02-13T05:15:21.987789Z","iopub.status.idle":"2022-02-13T05:15:21.997298Z","shell.execute_reply.started":"2022-02-13T05:15:21.987748Z","shell.execute_reply":"2022-02-13T05:15:21.996495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CNN Model","metadata":{}},{"cell_type":"code","source":"fold = 0\ncv_scores = []\nfor train_n_validation_indexes, test_indexes in k_fold.split(x_corpus, y_corpus_raw):\n    x_train_n_validation = x_corpus[train_n_validation_indexes]\n    y_train_n_validation = y_corpus[train_n_validation_indexes]\n    x_test = x_corpus[test_indexes]\n    y_test = y_corpus[test_indexes]\n\n    # train and validation data sets\n#     x_train, x_valid, y_train, y_valid = train_test_split(x_train_n_validation, y_train_n_validation,\n#                                                           test_size=VALIDATION_TEST_SIZE, random_state=94)\n\n    n_timesteps = len(x_train_n_validation)\n    n_features = len(x_train_n_validation[0])\n    # ################## Deep Neural Network Model ###################### #\n    model = Sequential()\n    model.add(Embedding(input_dim=dictionary_length, output_dim=60, input_length=max_word_count))\n    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n    model.add(MaxPooling1D())\n    model.add(Flatten())\n    model.add(Dense(250, activation='relu'))\n    model.add(Dense(2, activation='sigmoid'))\n#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    adam_optimizer = Adam(lr=0.001, decay=0.0001)\n    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=[metrics.CategoricalAccuracy(), metrics.AUC(), metrics.Precision(), metrics.Recall()])\n    \n    print(model.summary())\n    # ################## Deep Neural Network Model ###################### #\n\n    best_accuracy = 0\n    best_loss = 100000\n    best_epoch = 0\n\n    epoch_history = {\n        'accuracy': [],\n        'val_accuracy': [],\n        'loss': [],\n        'val_loss': [],\n        'auc': [],\n        'val_auc': []\n    }\n\n    history = model.fit(x=x_train_n_validation, y=y_train_n_validation, epochs=MAX_EPOCHS, batch_size=32,\n                            verbose=1, shuffle=True)\n    scores = model.evaluate(x_test, y_test, verbose=0)\n    print(model.metrics_names)\n    print(\"scores:\", scores)\n    cv_scores.append(scores)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:15:21.998528Z","iopub.execute_input":"2022-02-13T05:15:21.999023Z","iopub.status.idle":"2022-02-13T05:16:25.510113Z","shell.execute_reply.started":"2022-02-13T05:15:21.998984Z","shell.execute_reply":"2022-02-13T05:16:25.509242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_accuracy = 0\nmean_auc = 0\nmean_precision = 0\nmean_recall = 0\nfor score in cv_scores:\n    mean_accuracy += score[1]\n    mean_auc += score[2]\n    mean_precision += score[3]\n    mean_recall += score[4]\nmean_accuracy = mean_accuracy / FOLDS_COUNT\nmean_auc = mean_auc / FOLDS_COUNT\nmean_precision = mean_precision / FOLDS_COUNT\nmean_recall = mean_recall / FOLDS_COUNT\nmean_f_score = 2*mean_precision*mean_recall/(mean_precision + mean_recall)\nprint(\"mean_accuracy\", mean_accuracy)\nprint(\"mean_auc\", mean_auc)\nprint(\"mean_precision\", mean_precision)\nprint(\"mean_recall\", mean_recall)\nprint(\"mean_f_score\", mean_f_score)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:16:25.511656Z","iopub.execute_input":"2022-02-13T05:16:25.511995Z","iopub.status.idle":"2022-02-13T05:16:25.52193Z","shell.execute_reply.started":"2022-02-13T05:16:25.511959Z","shell.execute_reply":"2022-02-13T05:16:25.52112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BiGRU Model","metadata":{}},{"cell_type":"code","source":"fold = 0\ncv_scores = []\nfor train_n_validation_indexes, test_indexes in k_fold.split(x_corpus, y_corpus_raw):\n    x_train_n_validation = x_corpus[train_n_validation_indexes]\n    y_train_n_validation = y_corpus[train_n_validation_indexes]\n    x_test = x_corpus[test_indexes]\n    y_test = y_corpus[test_indexes]\n\n\n    n_timesteps = len(x_train_n_validation)\n    n_features = len(x_train_n_validation[0])\n    # ################## Deep Neural Network Model ###################### #\n    model = Sequential()\n    model.add(Embedding(input_dim=dictionary_length, output_dim=60, input_length=max_word_count))\n    model.add(GRU(units=256, return_sequences=True))\n    model.add(MaxPooling1D())\n    model.add(Flatten())\n    model.add(Dense(250, activation='relu'))\n    model.add(Dense(2, activation='softmax'))\n#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    adam_optimizer = Adam(lr=0.001, decay=0.0001)\n    model.compile(loss='categorical_crossentropy', optimizer=adam_optimizer, metrics=[metrics.CategoricalAccuracy(), metrics.AUC(), metrics.Precision(), metrics.Recall()])\n    \n#     print(model.summary())\n    # ################## Deep Neural Network Model ###################### #\n\n    best_accuracy = 0\n    best_loss = 100000\n    best_epoch = 0\n\n    epoch_history = {\n        'accuracy': [],\n        'val_accuracy': [],\n        'loss': [],\n        'val_loss': [],\n        'auc': [],\n        'val_auc': []\n    }\n\n    history = model.fit(x=x_train_n_validation, y=y_train_n_validation, epochs=MAX_EPOCHS, batch_size=30, verbose=1, shuffle=True)\n    scores = model.evaluate(x_test, y_test, verbose=0)\n    print(model.metrics_names)\n    cv_scores.append(scores)\n    print(\"scores:\", scores)\n    fold += 1","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:16:25.523168Z","iopub.execute_input":"2022-02-13T05:16:25.523787Z","iopub.status.idle":"2022-02-13T05:17:53.24152Z","shell.execute_reply.started":"2022-02-13T05:16:25.523647Z","shell.execute_reply":"2022-02-13T05:17:53.240636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_accuracy = 0\nmean_auc = 0\nmean_precision = 0\nmean_recall = 0\nfor score in cv_scores:\n    mean_accuracy += score[1]\n    mean_auc += score[2]\n    mean_precision += score[3]\n    mean_recall += score[4]\nmean_accuracy = mean_accuracy / FOLDS_COUNT\nmean_auc = mean_auc / FOLDS_COUNT\nmean_precision = mean_precision / FOLDS_COUNT\nmean_recall = mean_recall / FOLDS_COUNT\nmean_f_score = 2*mean_precision*mean_recall/(mean_precision + mean_recall)\nprint(\"mean_accuracy\", mean_accuracy)\nprint(\"mean_auc\", mean_auc)\nprint(\"mean_precision\", mean_precision)\nprint(\"mean_recall\", mean_recall)\nprint(\"mean_f_score\", mean_f_score)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:17:53.24314Z","iopub.execute_input":"2022-02-13T05:17:53.243522Z","iopub.status.idle":"2022-02-13T05:17:53.254795Z","shell.execute_reply.started":"2022-02-13T05:17:53.243482Z","shell.execute_reply":"2022-02-13T05:17:53.253888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble Model","metadata":{}},{"cell_type":"code","source":"def get_lstm_model():\n    model = Sequential()\n    model.add(Embedding(input_dim=dictionary_length, output_dim=60, input_length=max_word_count))\n    model.add(LSTM(600))\n    model.add(Dense(units=max_word_count, activation='tanh', kernel_regularizer=regularizers.l2(0.04),\n                    activity_regularizer=regularizers.l2(0.015)))\n    model.add(Dense(units=max_word_count, activation='relu', kernel_regularizer=regularizers.l2(0.01),\n                    bias_regularizer=regularizers.l2(0.01)))\n    model.add(Dense(2, activation='softmax', kernel_regularizer=regularizers.l2(0.001)))\n    adam_optimizer = Adam(lr=0.001, decay=0.0001)\n    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=[metrics.CategoricalAccuracy(), metrics.AUC(), metrics.Precision(), metrics.Recall()])\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:27:03.923413Z","iopub.execute_input":"2022-02-13T05:27:03.923801Z","iopub.status.idle":"2022-02-13T05:27:03.931509Z","shell.execute_reply.started":"2022-02-13T05:27:03.923765Z","shell.execute_reply":"2022-02-13T05:27:03.930504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_cnn_model():\n    model = Sequential()\n    model.add(Embedding(input_dim=dictionary_length, output_dim=60, input_length=max_word_count))\n    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n    model.add(MaxPooling1D())\n    model.add(Flatten())\n    model.add(Dense(250, activation='relu'))\n    model.add(Dense(2, activation='sigmoid'))\n    adam_optimizer = Adam(lr=0.001, decay=0.0001)\n    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=[metrics.CategoricalAccuracy(), metrics.AUC(), metrics.Precision(), metrics.Recall()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:27:25.024526Z","iopub.execute_input":"2022-02-13T05:27:25.024945Z","iopub.status.idle":"2022-02-13T05:27:25.038361Z","shell.execute_reply.started":"2022-02-13T05:27:25.024906Z","shell.execute_reply":"2022-02-13T05:27:25.037273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_bi_gru_model():\n    model = Sequential()\n    model.add(Embedding(input_dim=dictionary_length, output_dim=60, input_length=max_word_count))\n    model.add(GRU(units=256, return_sequences=True))\n    model.add(MaxPooling1D())\n    model.add(Flatten())\n    model.add(Dense(250, activation='relu'))\n    model.add(Dense(2, activation='sigmoid'))\n    adam_optimizer = Adam(lr=0.001, decay=0.0001)\n    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=[metrics.CategoricalAccuracy(), metrics.AUC(), metrics.Precision(), metrics.Recall()])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:27:36.347376Z","iopub.execute_input":"2022-02-13T05:27:36.347699Z","iopub.status.idle":"2022-02-13T05:27:36.355238Z","shell.execute_reply.started":"2022-02-13T05:27:36.347668Z","shell.execute_reply":"2022-02-13T05:27:36.354068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom scipy import stats\ndef ensemble_pred(models, test_set):\n    predictions = []\n    for model in models:\n        pred = model.predict(test_set, batch_size = 32)\n        pred = list(np.argmax(pred, axis=1))\n#         print(pred)\n        predictions.append(pred)\n    predictions_np = np.array(predictions)\n    m = stats.mode(predictions_np)\n    return m.mode[0].tolist()","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:31:57.718923Z","iopub.execute_input":"2022-02-13T05:31:57.719257Z","iopub.status.idle":"2022-02-13T05:31:57.725292Z","shell.execute_reply.started":"2022-02-13T05:31:57.719224Z","shell.execute_reply":"2022-02-13T05:31:57.724477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\n\ndef evaluate_perf(pred, actual):\n    actual_list = [i[1] for i in actual]\n    accuracy = accuracy_score(actual_list, pred)\n    precision = precision_score(actual_list, pred, average='macro')\n    recall = recall_score(actual_list, pred, average='macro')\n    f_score = f1_score(actual_list, pred, average='macro')\n    auc = roc_auc_score(actual_list, pred)\n    return { \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f_score\": f_score, 'auc': auc}","metadata":{"execution":{"iopub.status.busy":"2022-02-13T05:32:41.127641Z","iopub.execute_input":"2022-02-13T05:32:41.127985Z","iopub.status.idle":"2022-02-13T05:32:41.13491Z","shell.execute_reply.started":"2022-02-13T05:32:41.127951Z","shell.execute_reply":"2022-02-13T05:32:41.133741Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy\nfrom sklearn.metrics import classification_report\n\ncvscores = []\nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\ny_corpus_raw = [0 if cls[1] == 1 else 1 for cls in y_corpus]\n\nfor train_n_validation_indexes, test_indexes  in kfold.split(x_corpus, y_corpus_raw):\n    x_train_n_validation = x_corpus[train_n_validation_indexes]\n    y_train_n_validation = y_corpus[train_n_validation_indexes]\n    x_test = x_corpus[test_indexes]\n    y_test = y_corpus[test_indexes]\n\n    # train and validation data sets\n#     x_train, x_valid, y_train, y_valid = train_test_split(x_train_n_validation, y_train_n_validation,\n#                                                           test_size=VALIDATION_TEST_SIZE, random_state=94)\n    lstm_model = get_lstm_model()\n    cnn_model = get_cnn_model()\n    gru_model = get_bi_gru_model()\n#     MAX_EPOCHS\n    lstm_model.fit(x_train_n_validation, y_train_n_validation, epochs=MAX_EPOCHS, batch_size=10, verbose=1)\n    cnn_model.fit(x_train_n_validation, y_train_n_validation, epochs=MAX_EPOCHS, batch_size=10, verbose=1)\n    gru_model.fit(x_train_n_validation, y_train_n_validation, epochs=MAX_EPOCHS, batch_size=10, verbose=1)\n#     model.fit(x_train, y_train, epochs=3, batch_size=10, verbose=1)\n        # evaluate the model\n#     scores = model.evaluate(x_test, y_test, verbose=0)\n#     print(model.metrics_names)\n#     print(scores)\n#     print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n    prediction = ensemble_pred([lstm_model, cnn_model, gru_model], x_test)\n    print(classification_report(list(np.argmax(y_test, axis=1)), prediction, labels=[0, 1]))\n    scores = evaluate_perf(prediction, y_test)\n    print(\"score: \", scores)\n    cvscores.append(scores)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T06:18:01.378429Z","iopub.execute_input":"2022-02-13T06:18:01.378741Z","iopub.status.idle":"2022-02-13T06:42:48.191843Z","shell.execute_reply.started":"2022-02-13T06:18:01.378712Z","shell.execute_reply":"2022-02-13T06:42:48.1909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"acc = []\npre = []\nrecall = []\nfscore = []\nauc = []\nfor i in cvscores:\n    acc.append(i['accuracy'])\n    pre.append(i['precision'])\n    recall.append(i['recall'])\n    fscore.append(i['f_score'])\n    auc.append(i['auc'])\n\nprint(\"Accuracy:\", sum(acc)/len(acc))\nprint(\"Precision:\", sum(pre)/len(pre))\nprint(\"Recall:\", sum(recall)/len(recall))\nprint(\"fScore:\", sum(fscore)/len(fscore))\nprint(\"auc:\", sum(auc)/len(auc))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T06:46:05.644167Z","iopub.execute_input":"2022-02-13T06:46:05.644518Z","iopub.status.idle":"2022-02-13T06:46:05.655114Z","shell.execute_reply.started":"2022-02-13T06:46:05.644486Z","shell.execute_reply":"2022-02-13T06:46:05.6542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}