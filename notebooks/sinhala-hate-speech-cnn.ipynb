{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport logging\nimport sys\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom keras import regularizers\nfrom keras import metrics\nfrom keras.layers import Dense, Conv1D, Dropout, MaxPooling1D, Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.preprocessing import sequence\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nimport emoji\nimport re\nlogging.basicConfig(format='%(levelname)s %(asctime)s: %(message)s', level=logging.INFO)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-04T13:03:55.84863Z","iopub.execute_input":"2021-09-04T13:03:55.849028Z","iopub.status.idle":"2021-09-04T13:04:01.37905Z","shell.execute_reply.started":"2021-09-04T13:03:55.848944Z","shell.execute_reply":"2021-09-04T13:04:01.378214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"../input/sinhala-hate-speech/final.csv\")\nprint(\"Before\", data.shape)\ndata = data[pd.notnull(data['full_text_without_emoji'])]\ndata = data[pd.notnull(data['label'])]\nprint(\"After:\", data.shape)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:01.380496Z","iopub.execute_input":"2021-09-04T13:04:01.380794Z","iopub.status.idle":"2021-09-04T13:04:01.658172Z","shell.execute_reply.started":"2021-09-04T13:04:01.380761Z","shell.execute_reply":"2021-09-04T13:04:01.657169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize(text: str) -> list:\n    # text characters to split is from: https://github.com/madurangasiriwardena/corpus.sinhala.tools\n    emojis = ''.join(emj for emj in emoji.UNICODE_EMOJI.keys())\n    return [token for token in\n            re.split(r'[.…,‌ ¸‚\\\"/|—¦”‘\\'“’´!@#$%^&*+\\-£?˜()\\[\\]{\\}:;–Ê  �‪‬‏0123456789' + emojis + ']', text)\n            if token != \"\"]\ntokenize(\"අනේ හුකන්න කියපන් උන්ට\")","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:01.660112Z","iopub.execute_input":"2021-09-04T13:04:01.660491Z","iopub.status.idle":"2021-09-04T13:04:01.668229Z","shell.execute_reply.started":"2021-09-04T13:04:01.660452Z","shell.execute_reply":"2021-09-04T13:04:01.667278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_corpus(corpus: list) -> list:\n    return [tokenize(text) for text in corpus]\ntokenized_corpus = tokenize_corpus([\"අනේ හුකන්න කියපන් උන්ට\", \"පාහර බැල්ලි එනෝ මෙතන මට වැඩ කියාදෙන්න හෙළුවෙන්\"])\ntokenized_corpus","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:01.670157Z","iopub.execute_input":"2021-09-04T13:04:01.670862Z","iopub.status.idle":"2021-09-04T13:04:01.680187Z","shell.execute_reply.started":"2021-09-04T13:04:01.670823Z","shell.execute_reply":"2021-09-04T13:04:01.679102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_WORD_COUNT = 60 \nDATA_SET_CLASSES = {\n    0: [0, 1],\n    1: [1, 0]\n}","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:01.681672Z","iopub.execute_input":"2021-09-04T13:04:01.682052Z","iopub.status.idle":"2021-09-04T13:04:01.688059Z","shell.execute_reply.started":"2021-09-04T13:04:01.682018Z","shell.execute_reply":"2021-09-04T13:04:01.686794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_class_to_one_hot_representation(classes: list):\n    return np.array([DATA_SET_CLASSES[cls] for cls in classes])\ntransform_class_to_one_hot_representation([1,0,1])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:01.689766Z","iopub.execute_input":"2021-09-04T13:04:01.690158Z","iopub.status.idle":"2021-09-04T13:04:01.700535Z","shell.execute_reply.started":"2021-09-04T13:04:01.690124Z","shell.execute_reply":"2021-09-04T13:04:01.69939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_dictionary(corpus_token: list) -> dict:\n    word_frequency = {}\n    dictionary = {}\n\n    for tweet in corpus_token:\n        for token in tweet:\n            if token in word_frequency:\n                word_frequency[token] += 1\n            else:\n                word_frequency[token] = 1\n\n    frequencies = list(word_frequency.values())\n    unique_words = list(word_frequency.keys())\n\n    # sort words by its frequency\n    frequency_indexes = np.argsort(frequencies)[::-1]  # reverse for descending\n    for index, frequency_index in enumerate(frequency_indexes):\n        # 0 is not used and 1 is for UNKNOWN\n        dictionary[unique_words[frequency_index]] = index + 2\n\n    return dictionary\nprint(tokenized_corpus)\ndictionary = build_dictionary(tokenized_corpus)\ndictionary","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:01.702409Z","iopub.execute_input":"2021-09-04T13:04:01.70285Z","iopub.status.idle":"2021-09-04T13:04:01.71753Z","shell.execute_reply.started":"2021-09-04T13:04:01.70281Z","shell.execute_reply":"2021-09-04T13:04:01.71651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transform_to_dictionary_values(corpus_token: list, dictionary: dict) -> list:\n    x_corpus = []\n    for tweet in corpus_token:\n        # 1 is for unknown (not in dictionary)\n        x_corpus.append([dictionary[token] if token in dictionary else 1 for token in tweet])\n\n    return x_corpus\ntransform_to_dictionary_values(tokenized_corpus, dictionary)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:01.720766Z","iopub.execute_input":"2021-09-04T13:04:01.721208Z","iopub.status.idle":"2021-09-04T13:04:01.730888Z","shell.execute_reply.started":"2021-09-04T13:04:01.721167Z","shell.execute_reply":"2021-09-04T13:04:01.729805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_set = data.values","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:01.733131Z","iopub.execute_input":"2021-09-04T13:04:01.733572Z","iopub.status.idle":"2021-09-04T13:04:01.747454Z","shell.execute_reply.started":"2021-09-04T13:04:01.733531Z","shell.execute_reply":"2021-09-04T13:04:01.746431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_SET_TEXT = 40\nlogging.info(\"Tokenizing the corpus\")\ncorpus_token = tokenize_corpus(data_set[:, DATA_SET_TEXT])","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:01.749058Z","iopub.execute_input":"2021-09-04T13:04:01.749541Z","iopub.status.idle":"2021-09-04T13:04:01.807172Z","shell.execute_reply.started":"2021-09-04T13:04:01.749498Z","shell.execute_reply":"2021-09-04T13:04:01.806229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.info(\"Building the dictionary\")\ndictionary = build_dictionary(corpus_token)\ndictionary_length = len(dictionary) + 2  # 0 is not used and 1 is for UNKNOWN\ndictionary_length","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:01.808469Z","iopub.execute_input":"2021-09-04T13:04:01.808809Z","iopub.status.idle":"2021-09-04T13:04:01.845633Z","shell.execute_reply.started":"2021-09-04T13:04:01.808773Z","shell.execute_reply":"2021-09-04T13:04:01.844621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logging.info(\"Transforming the corpus to dictionary values\")\nx_corpus = transform_to_dictionary_values(corpus_token, dictionary)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:01.84687Z","iopub.execute_input":"2021-09-04T13:04:01.847203Z","iopub.status.idle":"2021-09-04T13:04:02.024948Z","shell.execute_reply.started":"2021-09-04T13:04:01.847167Z","shell.execute_reply":"2021-09-04T13:04:02.022836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DATA_SET_CLASS = 29\ny_corpus = transform_class_to_one_hot_representation(data_set[:, DATA_SET_CLASS])\ny_corpus","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:02.026678Z","iopub.execute_input":"2021-09-04T13:04:02.027153Z","iopub.status.idle":"2021-09-04T13:04:02.052402Z","shell.execute_reply.started":"2021-09-04T13:04:02.027116Z","shell.execute_reply":"2021-09-04T13:04:02.05144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_corpus = sequence.pad_sequences(x_corpus, maxlen=MAX_WORD_COUNT )\nlen(x_corpus)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:02.053827Z","iopub.execute_input":"2021-09-04T13:04:02.05436Z","iopub.status.idle":"2021-09-04T13:04:02.104421Z","shell.execute_reply.started":"2021-09-04T13:04:02.05432Z","shell.execute_reply":"2021-09-04T13:04:02.103428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ################## Deep Neural Network ###################### #\nFOLDS_COUNT = 10\nMAX_EPOCHS = 3\nVALIDATION_TEST_SIZE = 0.12\nmax_word_count = MAX_WORD_COUNT\n\n# splitting data for 5-fold cross validation\nk_fold = StratifiedKFold(n_splits=FOLDS_COUNT, shuffle=True, random_state=18)\n# to split, raw format (integer) is required\ny_corpus_raw = [0 if cls[1] == 1 else 1 for cls in y_corpus]","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:02.105644Z","iopub.execute_input":"2021-09-04T13:04:02.106098Z","iopub.status.idle":"2021-09-04T13:04:02.11695Z","shell.execute_reply.started":"2021-09-04T13:04:02.106057Z","shell.execute_reply":"2021-09-04T13:04:02.116204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fold = 0\ncv_scores = []\nfor train_n_validation_indexes, test_indexes in k_fold.split(x_corpus, y_corpus_raw):\n    x_train_n_validation = x_corpus[train_n_validation_indexes]\n    y_train_n_validation = y_corpus[train_n_validation_indexes]\n    x_test = x_corpus[test_indexes]\n    y_test = y_corpus[test_indexes]\n\n    # train and validation data sets\n    x_train, x_valid, y_train, y_valid = train_test_split(x_train_n_validation, y_train_n_validation,\n                                                          test_size=VALIDATION_TEST_SIZE, random_state=94)\n\n    n_timesteps = len(x_train)\n    n_features = len(x_train[0])\n    # ################## Deep Neural Network Model ###################### #\n    model = Sequential()\n    model.add(Embedding(input_dim=dictionary_length, output_dim=60, input_length=max_word_count))\n    model.add(Conv1D(32, 3, padding='same', activation='relu'))\n    model.add(MaxPooling1D())\n    model.add(Flatten())\n    model.add(Dense(250, activation='relu'))\n    model.add(Dense(2, activation='sigmoid'))\n#     model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    adam_optimizer = Adam(lr=0.001, decay=0.0001)\n    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=[metrics.CategoricalAccuracy(), metrics.AUC(), metrics.Precision(), metrics.Recall()])\n    \n    print(model.summary())\n    # ################## Deep Neural Network Model ###################### #\n\n    best_accuracy = 0\n    best_loss = 100000\n    best_epoch = 0\n\n    epoch_history = {\n        'accuracy': [],\n        'val_accuracy': [],\n        'loss': [],\n        'val_loss': [],\n        'auc': [],\n        'val_auc': []\n    }\n\n    history = model.fit(x=x_train_n_validation, y=y_train_n_validation, epochs=MAX_EPOCHS, batch_size=32,\n                            verbose=0, shuffle=True)\n    scores = model.evaluate(x_test, y_test, verbose=0)\n    print(model.metrics_names)\n    print(\"scores:\", scores)\n    cv_scores.append(scores)\n    # for each epoch\n#     for epoch in range(MAX_EPOCHS):\n#         logging.info(\"Fold: %d/%d\" % (fold, FOLDS_COUNT))\n#         logging.info(\"Epoch: %d/%d\" % (epoch, MAX_EPOCHS))\n#         history = model.fit(x=x_train, y=y_train, epochs=1, batch_size=1, validation_data=(x_valid, y_valid),\n#                             verbose=1, shuffle=True)\n#         print(\"history:\", history.history)\n        # get validation (test) accuracy and loss\n#         accuracy = history.history['val_auc'][0]\n#         loss = history.history['val_loss'][0]\n\n#         # set epochs' history\n#         epoch_history['auc'].append(history.history['auc'][0])\n#         epoch_history['val_auc'].append(history.history['val_auc'][0])\n#         epoch_history['loss'].append(history.history['loss'][0])\n#         epoch_history['val_loss'].append(history.history['val_loss'][0])\n\n        # select best epoch and save to disk\n#         if accuracy >= best_accuracy and loss < best_loss + 0.01:\n#             logging.info(\"Saving model\")\n#             model.save(\"%s/model_fold_%d.h5\" % (directory, fold))\n\n#             best_accuracy = accuracy\n#             best_loss = loss\n#             best_epoch = epoch\n        # end of epoch\n\n    # Plot training & validation accuracy values\n#     plt.plot(epoch_history['auc'])\n#     plt.plot(epoch_history['val_auc'])\n#     plt.title('Model AUC')\n#     plt.ylabel('AUC')\n#     plt.xlabel('Epoch')\n#     plt.legend(['Train', 'Validation'], loc='upper left')\n# #     plt.savefig(\"%s/plot_model_accuracy_%d\" % (directory, fold))\n#     plt.show()\n\n#     # Plot training & validation loss values\n#     plt.plot(epoch_history['loss'])\n#     plt.plot(epoch_history['val_loss'])\n#     plt.title('Model loss')\n#     plt.ylabel('Loss')\n#     plt.xlabel('Epoch')\n#     plt.legend(['Train', 'Validation'], loc='upper left')\n# #     plt.savefig(\"%s/plot_model_loss_%d\" % (directory, fold))\n#     plt.show()\n    fold += 1","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:04:02.11828Z","iopub.execute_input":"2021-09-04T13:04:02.118673Z","iopub.status.idle":"2021-09-04T13:05:12.55352Z","shell.execute_reply.started":"2021-09-04T13:04:02.118604Z","shell.execute_reply":"2021-09-04T13:05:12.552642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.fit(x=x_train, y=y_train, epochs=1 validation_data=(x_test, y_test),\n#                             verbose=1, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:05:12.554869Z","iopub.execute_input":"2021-09-04T13:05:12.555179Z","iopub.status.idle":"2021-09-04T13:05:12.561049Z","shell.execute_reply.started":"2021-09-04T13:05:12.55515Z","shell.execute_reply":"2021-09-04T13:05:12.560119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_accuracy = 0\nmean_auc = 0\nmean_precision = 0\nmean_recall = 0\nfor score in cv_scores:\n    mean_accuracy += score[1]\n    mean_auc += score[2]\n    mean_precision += score[3]\n    mean_recall += score[4]\nmean_accuracy = mean_accuracy / FOLDS_COUNT\nmean_auc = mean_auc / FOLDS_COUNT\nmean_precision = mean_precision / FOLDS_COUNT\nmean_recall = mean_recall / FOLDS_COUNT\nmean_f_score = 2*mean_precision*mean_recall/(mean_precision + mean_recall)\nprint(\"mean_accuracy\", mean_accuracy)\nprint(\"mean_auc\", mean_auc)\nprint(\"mean_precision\", mean_precision)\nprint(\"mean_recall\", mean_recall)\nprint(\"mean_f_score\", mean_f_score)","metadata":{"execution":{"iopub.status.busy":"2021-09-04T13:05:12.562571Z","iopub.execute_input":"2021-09-04T13:05:12.562987Z","iopub.status.idle":"2021-09-04T13:05:12.573256Z","shell.execute_reply.started":"2021-09-04T13:05:12.562948Z","shell.execute_reply":"2021-09-04T13:05:12.572277Z"},"trusted":true},"execution_count":null,"outputs":[]}]}